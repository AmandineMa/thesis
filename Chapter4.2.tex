\ifdefined\included
\else
\documentclass[a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
	\setcounter{chapter}{8} %% Numéro du chapitre précédent ;)
	\dominitoc
	\faketableofcontents
	\fi

\chapter{The Director Task: a Psychology-Inspired Task to Assess Cognitive and Interactive Robot Architectures}
\chaptermark{The director task}
\label{chapter:chap9}
\minitoc

In this chapter, we propose a new psychology-inspired task, gathering perspective-taking, planning, knowledge representation with theory of mind, manipulation, and communication. Along with a precise description of the task allowing its replication, we present a cognitive robot architecture able to perform it in its nominal cases. In addition, we suggest some challenges and evaluations for the Human-Robot Interaction research community, all derived from this easy-to-replicate task.

The contribution presented in this chapter is excerpted from our work, published in the proceedings of the RO-MAN 2021 conference~\citep{sarthou_2021_director}. This contribution has been achieved in collaboration with other PhD students of the \acrshort{hri} teams, our mutual thinking leading to the formulation of this new task for \acrshort{hri}. Then, more specifically in relation to the software implementation, Guilhem Buisan was concerned about the task planning part. Guillaume Sarthou worked on the knowledge management. Kathleen Belhassein has designed the presented task with us giving her psychologist point of view to create a task on which user studies could be performed. The engineer Yannick Riou worked on the motion planning component allowing us to develop a task where the robot acts on its environment. My involvement in this task was on the supervision component. It was an evolved version of the one which ran for the direction-giving task presented in the previous chapter, \ie the \acrshort{jahrvis} as described in Chapters~\ref{chapter:chap5} and~\ref{chapter:chap6}. It has also been the opportunity to refine the architecture developed for the \acrshort{mummer} project, leading to the one presented in this chapter.


\section{Introduction}

Developing robotic architectures adapted to Human-Robot Interaction and thus able to carry out interactions in an acceptable way is still today a real challenge. The complexity comes, among other things, from the number of capabilities that the robot must be endowed with and therefore from the number of software components which must be integrated in a consistent manner. Such architectures should provide the robot with the capability to perceive its environment and its partners, to merge and interpret this perceptual information, to communicate about it, to plan tasks with its partner, to estimate the others' perspective and mental state, etc. Once developed, the evaluation of these architectures can be difficult because all these components grouped into a single system. The tasks we usually want the robot to handle must highlight a maximum of abilities, while still being simple enough to be reproduced by the community. Moreover, we should be able to conduct user studies with it to validate choices regarding naive users.

Since a long term goal of the robotic field is to see robots evolving in our daily life, many tasks and scenarios have been inspired by everyday activities. Even if these tasks offer a large variety of situation to be handle, since the human partner is not limited in his actions, they have the disadvantage of not highlighting some subtle abilities which are nevertheless necessary for good interaction.
The robot guide task in mall~\citep{satake_2015_should}, museum, or airport, requires high communication skills to understand free queries (possibly involving chatting) and respond to them, whether to indicate a direction or to give advice. However, the perception needs can be limited due to the vast environments, as well as the perspective-taking needs due to the same perception of the environment by the robot and the human\footnote{For sure we can find some tricky cases where it could help but they do not reflect common situations.}. Finally, with such a task the human partner is not an actor of the task and just has to listen to the robot once their question is asked. Even if being in more constrained environments, bartender-like tasks~\citep{petrick_2012_social} have the same disadvantages. Indeed, the human is considered as a customer, and as such, the interaction with the robot is limited. The robot will never ask the human to help it for performing a task and their actions do not require coordination either full collaboration.

To involve the human partner in the task and requiring him to act with the robot, assembly-like tasks~\citep{tellex_2014_asking} can be used. Nevertheless, in most cases, the human acts as an assistant rather than as a partner as full collaboration can be challenging to perform. The robot thus elaborates a plan and performs the assemble, then asks for help when detecting errors during the execution (\eg when it cannot reach some pieces). Here the task leads to unidirectional communication. Moreover, because in such a task both the robot and the human have equivalent knowledge about the environment, it can be hard to design situations where belief divergence appears and thus perspective-taking would be required.

Scaling down an everyday task to transform it into a toy task around a table can reduce the task complexity and allows easy reproducibility. Moreover, it allows the robot and the human to work in the vicinity of each other, with smaller robots for example. With the toy version of the assembly task presented in~\citep{brawer_2018_situated}, the human is more involved in the task. They ask the robot to take pieces and to hold them to help them assemble a chair. Even if the communication is unidirectional, we could imagine inverting the roles to test different abilities. Moreover, communication implies objects referring with the use of various visual features about the entities. Even if both agents have the same knowledge about the environment, the communication is grounded according to the current state of the world. In this task, no decision has to be made by the robot but once again, inverting the roles could open other challenges.

To focus studies around perspective-taking and belief management, the Sally and Anne scenario, coming from a psychology test, has been studied in robotic~\citep{milliez_2014_framework}. In this scenario, the robot is an observer of a situation where two humans come and go from a room, and move an object from a box to another. Since a human is in the room when the other is acting, a belief divergence appears between the two humans and the robot has to understand it. While the task highlights the belief management, it is first limited regarding the perspective-taking since the human presence or not could be sufficient to estimate the humans beliefs\footnote{When both humans are in the room they have the same perception of the scene but have different beliefs about hidden objects. Perspective-taking would be required if the humans could lean over the boxes to check what is inside.}. Moreover, the humans do not act with the robot since it is just an observer of the scene. In addition, no goal is formulated and the human neither interacts with one another. Finally, no communication is needed in the task. The scenario is thus focussed on the analysis of a situation.

In this chapter, we first propose a new psychology-inspired task that we think to be challenging for the Human-Robot Interaction community and rich enough to be extended: the Director Task. Inter alia, it requires perspective-taking, planning, knowledge representation with theory of mind, manipulation, communication, and decision-making. Then, we present the robotic cognitive architecture that we develop to perform the task in its nominal cases. Finally, on the basis of the presented task and what has been developed, we present a discussion about the possible future challenges and evaluations for the research community, with possible extensions of the task.

\section{The Director Task: From psychology to Human-Robot Interaction}

In this section, we present the origins of the Director Task and the needs it aims to respond to regarding other tasks from the psychology. Then, we detail the setup we have designed in terms of objects characteristics and organization in the environment. We end this section with our adaptation and the required abilities we have identified.

\subsection{The original task}\label{chap9:subsec:psycho}

The Director Task has been mainly used in psychology as a test of the \acrfull{tom} usage in referential communication (see Section~\ref{chap1:sec:tom}). This task originates from a referential communication game from \cite{krauss_1977_social}. In this game, two participants are one in front of the other with an opaque panel between them. A speaker has to describe odd designs to a listener, either to number them for the adults or create a stack of cubes for the children. To refer to the odd figures, participants have to use images (\eg ``it looks like a plane'').

This game was then adapted by \citep{keysar_2000_taking} and became the Director Task. It has been used to study the influence of mutual knowledge in language comprehension. In this task, two people are placed one in front of the other but instead of an opaque panel between them, they place a vertical grid composed of different cells and objects in some of them. The \textbf{director}, a participant or in most cases an accomplice, instructs the \textbf{receiver}, a participant, about objects to move in the grid. The receiver thus follows the director's instructions about objects to move. The particularity of the task is that some cells are hidden from the director, meaning that the receiver, being on the other side of this grid, does not have the same perspective as the director. They thus know the content of more cells than the director and consequently sees more objects. When the director instructs the receiver to move an object, for a successful performance, participants must take the perspective of the director to move the right one. Because the configuration evolves all along with the task, they have to update their estimated perspective all along with the interaction.

\begin{figure}[ht!]
	\centering
	\includegraphics[scale=0.25]{figures/chapter4/dt_apple.png}
	\caption{\label{chap9:fig:dt_apple} Sample display from the director's and the receiver's perspectives. The asterisk indicates the target object. Giving the sentence ``the smallest apple'' the receiver should find the good one even if he can see a smallest one in its perspective. }
\end{figure}

For example in Figure~\ref{chap9:fig:dt_apple}, if the director asks for the smallest apple (*), the proper smallest (called competitor) is only visible by the participant and not by the director. The participant then must understand the director's perspective to take the target apple and not the competitor one. Some studies showed that for their first attempt, participants considered or took the smallest apple from their own point of view and only after, the target one. These results were interpreted in various work as the participants understanding language in an egocentric way~\citep{keysar_1994_illusory, keysar_1998_egocentric, keysar_2002_self, keysar_2003_limits}. 
Some social cognition studies used a computer-version of the Director Task whose results are consistent with the ones mentioned previously, namely that participants do not use \acrshort{tom} inferences in language interpretation~\citep{dumontheil_2010_online}.

\cite{santiesteban_2012_training} considered in their study that perspective-taking abilities were measured by the Director Task whereas \acrshort{tom} usage was investigated through another task called ``strange stories''~\citep{happe_1994_advanced}. However, this \acrshort{tom} task requires the attribution of mental states to a story protagonist (to have knowledge of others' mental states), whereas the Director Task asks for adopting the perspective of the director in order to follow their instructions (to use this knowledge in order to execute the task properly). Thus, the authors estimated that the Director Task requires a higher degree of self-other distinction by continuously isolating our own perspective from the director one. In addition to perspective-taking abilities, the Director Task makes use of executive functions~\citep{rubio_2017_director} and attentional resources~\citep{lin_2010_reflexively}.

The Director Task has thus been particularly used in psychology studies of referential communication, language comprehension, and perspective-taking abilities. However, to date it has never been exploited in the context of a HRI although this task presents interesting challenges for this field. It would not only bring technical challenges but also provide a way to investigate the different cognitive and behavioral processes involved in such a cooperative Human-Robot task.

\subsection{The Director Task setup}\label{chap9:subsec:material}

The material used in this task has been chosen to be easily acquired and can be hand-built. It is composed of blocks, compartments, and a storage area. Each element is equipped with AR-tags allowing the robot to perceive them without advanced perception algorithms.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{figures/chapter4/setup.png}
	\caption{\label{chap9:fig:setup} A director task setup adapted to the \acrshort{hri} with the director's and receiver's perspectives. For the material, each element (blocks and compartment) is equipped with AR-tags allowing their detection by the robot. Each block has four visual characteristics: a main color, a border color, a geometric figure, and a figure color. Compartments can be hidden for the director or the receiver. For the director to designate the block marked with a red circle, estimating the receiver's perspective, he can refer to it by its main color (blue) because he estimates the other blue block is not visible by the receiver. For the receiver, by taking into account the director's perspective, he can understand the referred block as he estimates the other blue block to not be visible by the director.}
\end{figure}

As shown in Figure~\ref{chap9:fig:setup}, the blocks have a primary color covering them all. On two opposite faces, additional visual features are drawn. The top part of these faces is dedicated to the robot's perception with unique AR-tag on each face\footnote{because the tags are different on each side, the director can not refer to them as the receiver does not see the same ones}. The bottom part is the same on both faces and is dedicated to the human perception with a main color, a border, and a geometric figure. Every visual feature (the colors and the forms) has exactly two variants. The colors are either blue or green and the figures are either a triangle or a circle.
The figures and colors have been chosen in such a way to allow the emergence of ``coded words'' between the participant to identify a block. With a bit of imagination, some could refer to the left-most block through the sentence ``the mountain in the sea'' or the second leftmost by ``the puddle''.
The number of features has been chosen to have sixteen block variants from which we remove the four uni-color variants (all the elements having the same color) to avoid too easy description of the kind ``the fully green block''.
Regarding their description complexity, while the main color is directly related to a block, the other colors are respectively related to the border and the figure. This means that for two blocks whose only difference is the color of one of these elements, the said element has to be referred to by its color. A description of a block involving all its features would be ``the [color] block with the [color] border and the [color] [figure]''. Such complete descriptions are hard for the human to process. In this way we expect the participants to minimize the complexity of their communication by referring to the blocks only using the features distinguishing them from other blocks.

Three types of compartment exist. Some are open on two of their opposite sides allowing both the receiver and director to see the content and to manipulate it. Some are open only on one of their sides meaning that only one of the participants can see and take what is inside. The other participant can thus neither know if a block is inside or not. The last compartment type has an open side and the opposite one equipped with a wire mesh. Because of the side with the wire mesh, both participants can see what is inside but only one of them can take it. With these three types, we will be able to test the impact of the awareness of the blocks (\eg a block is known to be present but not necessarily visible), the visibility of the blocks, and their reachability (\eg a block can be visible but not reachable).

Finally, one storage area, corresponding to the place where the receiver has to store the blocks, is delimited by a rectangle on a shelf.

\begin{figure}[ht!]
	\centering
	\includegraphics[scale=0.15]{figures/chapter4/positions.png}
	\caption{\label{chap9:fig:positions} The Director Task setup with the robot and the human partner one in front of the other and a piece of furniture between them. Compartments are placed on top of the furniture and blocks are placed in the compartments. Next to the agent having the receiver role, here the human, a storage area is placed to drop the removed blocks. }
\end{figure}

Regarding the disposition, the compartments are stacked on a piece of furniture to create a kind of grid. The blocks can be put in a compartment. As illustrated in Figure~\ref{chap9:fig:positions}, the two agents are placed one in front of the other with the furniture and thus the compartments between them. Finally, one storage area, corresponding to the place where the receiver has to store the blocks, is delimited by a rectangle on a shelf next to the receiver. In the figure, the human would be the receiver since he has the storage area on his right.

\subsection{The Director Task adaptation for HRI}

In this section, we present the DT-HRI, the Director Task as we designed it for HRI, keeping the principle of two participants with a vertical grid between them. The high-level goal of the task is known by both agents: to put a set of blocks away. The precise goal is given by the experimenter to the director, either the robot or the human, i.e., the set of blocks that the receiver should remove from the compartments (see Figure~\ref{chap9:fig:setup}).

As mentioned in the previous section, the Director Task characteristics bring a number of interesting  challenges for a collaborative robot to solve. Because this is a task with two roles, one of the first challenges is to build a robotic architecture that gives the robot the ability to play both roles. Then, each role brings some problems to solve from a robotic point of view. 

In the original task, the director knows they have a subset of the receiver's perspective, they can consider all the objects when communicating. Thus, only the receiver has to reason about the other's perspective, taking into account that some objects are not visible by the director. In order to enrich the task for HRI application, we propose to also have compartments hidden from the receiver and visible by the director (see Figure~\ref{chap9:fig:setup}). Therefore, both roles have to perform perspective-taking, whether to give instructions or to understand them.
On one hand, this challenging task allows to demonstrate the abilities of a robotic system. On the other hand, it is an easily reproducible scenario to perform user studies on human-robot interactions in a controlled environment. 

To be able to study more specifically some skills, such as verbal communication, perspective-taking and adaptation, we defined a set of rules for both the robot and the participant. First, to focus the task on verbal communication, the agents are \textbf{not allowed to point} to objects, either with their hand or gaze. Then, to strengthen the perspective-taking aspect and not fall into a simple referential communication task, participants are \textbf{not allowed to use geometrical relations} in the verbal communications. They cannot, for example, say ``the leftmost block'' or ``the block to the right of the green one''. In this way they are limited to few visual features, with high ambiguity, therefore requiring to take into account the other perspective. Finally, to enable an evolution of the situation over time and thus requiring a constant adaptation during the interaction, the objects are not moved from one compartment to another but removed from the compartments. The \textbf{order of the instructions is free}, enabling the director to elaborate a strategy if needed.

\subsection{A task to demonstrate the abilities of a robotic system}\label{subsec:abilities}

More than being an easily reproducible scenario to perform user studies on human-robot interactions in a controlled environment, the Director Task allows to demonstrate abilities of a robotic system. We detail here some abilities for which the task has been designed for.

\paragraph{Perspective-taking abilities} When working on the \acrshort{tom} in the HRI context, the Sally-Anne test has been used multiple times and allowed to demonstrate some systems~\citep{milliez_2014_framework}. But, one of the benefits of the Director Task compared to the Sally-Anne test is that the agents (human or robot/director and receiver) have not only to infer knowledge using the other's point of view but also to act so it is possible to acknowledge that they use it in decision-making. 

\paragraph{Communication abilities} Moreover, the task 
requires to put a focus on communications which is widely studied in HRI. Indeed, the communication about an object can be more or less efficient, depending on the number of characteristics given about the object or the pertinence of these characteristics (\eg in Figure~\ref{chap9:fig:dt_apple}, the director does not need to add ``red'' to ``take the small apple'' as there is no apple of a different color). The robot needs to be able to give proper instructions but also to understand the human ones.

\paragraph{Planning abilities} When a large number of blocks has to be taken in the task goal, it quickly becomes complicated to communicate about some of them as the director would have to add a lot of adjectives to be able to refer to one block. Therefore when the robot is the director, it becomes interesting to integrate the communication and the task planning together. Indeed, depending on the order in which the blocks are designated, the complexity of instructions can decrease or increase. Then, the planner can return an optimal order in which the robot has to give the instructions to the human.

\paragraph{Contingencies handling abilities} While performing the Director Task, errors can happen. Either because the director gives a wrong instruction or the receiver misunderstands the instruction and takes the wrong block. In both cases, it can be because of a wrong consideration of the other agent's perspective. In the latter case, the instruction might be right but hard to interpret by the receiver leading to an error from them. Finally, errors can happen because of a failed action execution (\eg a block falls on the floor), a system failure for the robot, inattention from the human, etc. A robot with a robust decision-making system will be able to analyze, try to determine their origin, and handle a number of these contingencies. For example, if the human takes the wrong block, the robot can react in different ways, \eg asking the human to put it back or saying nothing and re-planning if this block was among the ones to take. If errors happen repeatedly, the robot can react differently than for a punctual error.


\section{The cognitive robot architecture}
\label{chap9:sec:archi}

In this section, we present the architecture developed to handle the Director Task in its nominal case but also to allow for future extensions, endowing the robot with the abilities described in section~\ref{subsec:abilities}. The architecture is basically the one presented in Section~\ref{chap3:sec:rob_archi}.
The seven identified modules are represented in Figure~\ref{chap9:fig:architecture} with their respective communication links. In the rest of the section, we detail each module and how we have refined them in terms of functionality and linking.

\subsection{Storing and reasoning on symbolic statements}

As seen in the previous chapters, knowledge allows the robot to understand the environment it evolves in. Moreover, this same knowledge makes the robot able to communicate with its human partner about the current state of the world and ground the partner's utterance regarding this same world state.

Some have chosen to propagate their knowledge all along their architecture~\citep{hawes_2007_balt}, each component enriching this knowledge at each stage. Others have preferred to see their knowledge base as an active server activating perception process regarding the searched information when needed~\citep{beetz_2018_know}.

As the architecture on which we based ours, we chose a central, server-based, knowledge base. We however refined it into two distinct sub-modules, the semantic knowledge base and the episodic one, as presented in Chapter~\ref{chapter:chap6}, managed by Ontologenius.

\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.40]{figures/chapter4/architecture.png}
	\caption{\label{chap9:fig:architecture} An overview of the architecture developed to handle the Director Task. Each block does not necessarily represent one software component but rather an architectural module (in terms of the features it implements). The arrows represent the type of information exchanged between the modules.}
\end{figure}


\subsection{Assessing the world: from geometry to symbolism}

The role of the geometrical Situation Assessment module is first to gather different perceptual information and build an internal geometric representation of the world. From this world representation, the module then runs reasoners to interpret it in terms of symbolic statements between the objects themselves and between the involved agents and the objects. Doing so, the module only builds the robot's representation that does not necessarily reflect what the human partner believes about the world. This is the case with the occluded compartments. If a block is present in a compartment occluded from the human perspective, this block is not visible and thus unknown to the human and should not exist in their representation of the world. Here is the second role of our Situation Assessment module, estimating the human's perspective and building an estimation of their world representation. It is the first step allowing to implement the \acrshort{tom} principles (see Section~\ref{chap1:sec:tom}).

To implement this module, we have chosen the Underworld framework~\citep{lemaignan_2018_underworlds}. It has the advantage to not be monolithic. Its principle is to create a set of worlds, each working at a different granularity and integrating specific features. It allows easy reuse of existing modules and makes the core reasoning capabilities independent of the used perception modalities. The worlds' structure we use is represented in Figure~\ref{chap9:fig:uwds}. At the top, there are the perception modalities, here AR-tags~\citep{fiala_2005_artag} for the objects and motion capture (mocap) system for the human detection. For each perception system, we define a world. In these worlds, we can filter the perception data depending on the system used. For the mocap, the data is clean enough. For the AR-tags we apply first a motion filter to discard data acquired when the robot moves and a field of view (FOV) filter to discard data from the border of the camera because of distortions. Moreover, both perception worlds can use the knowledge base presented previously to get the entities' CAD models and unique identifiers (UIDs) shared across all the components of the architecture. When the AR-tags world receives an AR id, it can query the semantic knowledge base to get the UID related to this tag and get its CAD model. As the output of these worlds, we ensure to have clean data with UID related to the knowledge base.

\begin{figure}[b!]
	\centering
	\includegraphics[scale=0.12]{figures/chapter4/uwds/uwds.png}
	\caption{\label{chap9:fig:uwds} The world cascading structure of the geometrical situation assessment system. The two worlds at the top are build from the perception systems and filtered. The world of the middle merges the different perception information and computes symbolic facts on it. The world at the bottom is the estimation of the human world representation and is computed based on perspective-taking in the robot's world. Like for the world of the middle, symbolic facts are computed and sent to the semantic knowledge base.}
\end{figure}


The world of the middle in Figure~\ref{chap9:fig:uwds} is the robot's world representation. Information from the perception worlds is merged along with the static elements (the building walls) and the robot model.
%From this world, additional perception reasoning processes are applied for the objects that are no more visible in the way of~\cite{milliez_2014_framework}. If an entity is no more perceived in one of the previous worlds, the algorithm first tests if it should be in the robot's FOV. If so, the robot should see it. It thus tests if another entity could hide it. If not, the object is removed from the world representation and kept otherwise. 
From there, geometric reasoners are applied to extract symbolic facts. In the current version of the system, the computed facts are \textit{isOnTopOf}, for an object put on top of another, \textit{isInside}, for a block in a compartment, \textit{isVisibleBy}, assessing if an agent could see the object or not, and \textit{isReachableBy}, assessing if an object can be taken by an agent. All these facts are sent to the robot's semantic knowledge base, where reasoners will deduce further facts. For example, if a block is in a compartment, the compartment has the block inside (inverse property), and if this compartment is on top of the table, the block inside is also above the table (chain axiom).

While the previous world corresponds to the robot's representation, the one below aims at representing the partner's one. From the previous world, we compute a segmentation image from the human point of view and use it as a filtered perception world. This allows us to instantiate the same kind of world management process we used for the robot but this time for the human. In this way, we emulate their perception capability and the geometric reasoners can be run in the same way as previously. Symbolic facts are thus computed and sent to the human's semantic knowledge base. In the world of the bottom on Figure~\ref{chap9:fig:uwds}, we can see that the two blocks in the occluded compartments are not present in the human world. Here we make explicit the difference between an object that is unknown and an object that is known but not visible.


\subsection{Planning with symbolic facts}

The symbolic planners are divided into two categories: the domain-independent, planning high-level tasks, and the domain-dependant, specialized in solving precise problems. We first introduce the domain-specific ones and the domain-independent in a second time.% as it takes advantage of the previous ones.

\subsubsection{Solving precise problems}

Building a single monolithic planner could be an intractable challenge. Thus, we chose to consider a set of dedicated planners which could be reused from one system to another. In the current version of the system, only one specific planner has been identified. This planner is a \acrfull{reg} solver. Regarding the current symbolic state of the world, it aims at finding the minimal set of relations to communicate and allows the listener to identify a given entity. For example, wanting to refer to a block being the only one with a green triangle on it among the other, this planner can find that the only relations to communicate are the block's figure and the figure's color. With this information, the listener should be able to identify the referred block without ambiguity.

This planner is presented in~\citep{buisan_2020_efficient} which is based on a Uniform Cost Search algorithm and which is to the date the most efficient one in term of computation time. It work with an ontology, being the semantic knowledge base presented previously. Because the communication information it generates will be interpreted by the robot's partner, we chose to give the estimated human knowledge base as input to the planner. Thanks to this, the blocks unknown from the human --- i.e., hidden from them --- are not taken into account as they cannot lead to any ambiguity for the listener. Moreover, this planner can take some constraints as input, related to the property usability and the context of the communication. The usable properties constraint prevents some properties to be used in a referring expression. Indeed, the input ontology is not dedicated to the specific referring expression generation problem and contains additional knowledge used by other modules as the objects' CAD models or tag UIDs, that does not aim to be communicated. The communication context aims at representing relations assumed to be already known by the listener. For the Director Task, when the robot asks the human to take a block, it assumes they know it is only talking about objects above the table around which the robot and the human are interacting. The already stored blocks --- not on the table anymore --- are thus not taken into account in the communication. If needed the communication context can be refined, for example by defining that the robot --- and thus should the human as well --- will only consider visible blocks and reachable blocks.

\subsubsection{Planning for self and others}

In the context of a Human-Robot interaction, when planning how to perform a high-level task, one has to take into account the human's contribution. Our current task planner is \acrshort{hatpehda} mentioned in Section~\ref{chap6:sec:plan_handling}. This planner allows the robot to plan by emulating the human decision, action, and reaction processes. For the Director Task, emulating the human reaction to a given instruction enables the comparison between multiple blocks order, the communication of higher-level instructions to the human (\eg ask to withdraw rather than take then put down) and the balance between multiple communication modalities.

As at execution time the supervision uses the \acrshort{reg}, a domain-specific planner, and because the task planner uses the same type of knowledge representation, thus \acrshort{hatpehda} can use this planner during its planning process. In the current architecture, it can thus estimate the cost and the feasibility of referring communication by calling the \acrshort{reg}.

\subsection{Managing the interaction}

Based on the components presented above, \acrshort{jahrvis} manages the execution of Director Tasks, based on its processes presented in Chapters \ref{chapter:chap5} and~\ref{chapter:chap6}.


\section{Demonstration of the task nominal case}

In this section, we present the Director Task performed by a human and a robot, in a laboratory setting. Each agent takes both roles. The video of this demonstration is available at: \url{https://www.youtube.com/watch?v=jtSyZeqBkp0}. It runs with the supervision system presented in Chapters~\ref{chapter:chap4} and~\ref{chapter:chap5}, \acrshort{jahrvis}, excluding the \acrlong{ham} that was not developed at the time of the video yet. Therefore, the actions executed by the human are manually fed to \acrshort{jahrvis} by an operator external to the task.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{figures/chapter4/config.png}
	\caption{\label{chap9:fig:expe_config} Initial set-up of our Director Task demonstration from the robot perspective. One block is hidden from the human and one is hidden from the robot.}
\end{figure}

The initial set-up of our Director Task demonstration is presented in Figure~\ref{chap9:fig:expe_config}, from the robot perspective. There are six blocks, one hidden from the human and another one hidden from the robot. As explained in Section~\ref{chap9:subsec:material}, they can be distinguished by their color, their border color and their geometric figure which can be a triangle or a circle and which has a color as well. From the left to the right and from the top to the bottom their complete descriptions are:
\begin{bulletList}
	\item The green block with a green border and a blue triangle (only visible by the robot)
	\item The blue block with a blue border and a green circle
	\item The blue block with a blue border and a green triangle
	\item The green block with a green border and a blue circle
	\item The blue block with a green border and a blue triangle
	\item The green block with a blue border and a green circle (only visible by the human)
\end{bulletList}

\subsection{PR2 as the director}

The goal of the task, \ie the list of blocks the robot should ask the human to remove, was written in \acrshort{jahrvis} beforehand. The expected world state at the end of the Director Task was:
\begin{bulletList}
	\item The blue block with a blue border and a green circle is in the green storage box
	\item The green block with a green border and a blue circle is in the green storage box
	\item The blue block with a green border and a blue triangle is in the green storage box
\end{bulletList}


At the beginning of the task execution, this goal was sent to the planner which output a plan with the order in which the robot should ask the human to remove them. Indeed, \acrshort{hatpehda} computed this plan based on the \acrfull{reg} solver, trying to minimize the communication cost. Thus, the computed block order was:
\begin{enumList}
	\item The green block with a green border and a blue circle
	\item The blue block with a blue border and a green circle
	\item The blue block with a green border and a blue triangle
\end{enumList}

For each time the robot has to ask the human to take a block to put it on the storage box, \acrshort{jahrvis} relies on the \acrshort{reg} solver to find the best way, \ie the less costly way, to communicate about this block (and the box) in a unambiguous manner, taking the human perspective into account (see Figure~\ref{chap9:fig:h_view}). The algorithm was presented in Section~\ref{chap6:subsec:info_comm}.

\begin{figure}[!hbt]
	\centering
	\includegraphics[scale=0.5]{figures/chapter4/robot_view.png}
	\caption{\label{chap9:fig:h_view} Visualization of the human's estimated geometric world from a third-person view, computed by the Situation Assessment. Thus, the green cube with a blue triangle is absent from this view as it is not visible by the human.}
\end{figure}


\begin{figure}[!t]
	\centering
	\begin{subfloat}[The robot said ``can you put the green block in the green storage box?'', thus the human is taking it. The other green block is not visible by the human.]{\label{chap9:fig:director_1}
			\includegraphics[width=0.4\linewidth]{figures/chapter4/director_1.png}}\hfill
	\end{subfloat}
	\begin{subfloat}[The human's perspective computed by the Situation Assessment before the first block was removed.]{\label{chap9:fig:director_1_h}
			\includegraphics[width=0.4\linewidth]{figures/chapter4/director_1_h_view.png}}
	\end{subfloat}
	\begin{subfloat}[The robot said ``can you put the block with the circle in the green storage box?'', thus the human is taking it.]{\label{chap9:fig:director_2}\includegraphics[width=0.4\linewidth]{figures/chapter4/director_2.png}}\hfill
	\end{subfloat}
	\begin{subfloat}[The human's perspective computed by the Situation Assessment before the second block was removed.]{
			\label{chap9:fig:director_2_h}\includegraphics[width=0.4\linewidth]{figures/chapter4/director_2_h_view.png}}
	\end{subfloat}
	\begin{subfloat}[The robot said ```can you put the block with the blue triangle in the green storage box?'', thus the human is taking it. The other block with a blue triangle is not visible by the human.]{\label{chap9:fig:director_3}
			\includegraphics[width=0.4\linewidth]{figures/chapter4/director_3.png}}\hfill
	\end{subfloat}
	\begin{subfloat}[The human's perspective computed by the Situation Assessment before the third block was removed.]{\label{chap9:fig:director_3_h}
			\includegraphics[width=0.4\linewidth]{figures/chapter4/director_3_h_view.png}}
	\end{subfloat}
	\caption{The robot, as the director, informs the human of the blocks to remove from the self, one by one. On the left are images from a camera, from the the robot's side. On the right are screenshots of the Situation Assessment, showing the computed human's perspective.}
	\label{chap9:fig:director}
\end{figure}

Therefore in the video (from $24''$) -- this is also visible in Figure~\ref{chap9:fig:director}, the robot asks first ``can you put the green block in the green storage box?'' because there are two green blocks but one of them is not visible by the human so the robot has no need to disambiguate them (see Figures~\ref{chap9:fig:director_1} and~\ref{chap9:fig:director_1_h}). Then, it asks ``can you put the block with the circle in the green storage box?'' as there is only one block with a circle on it now that the other block with one was previously remove (see Figures~\ref{chap9:fig:director_2} and~\ref{chap9:fig:director_2_h}). Finally, it asks ``can you put the block with the blue triangle in the green storage box?''. Indeed, there are three blocks with a triangle but only two visible by the human, so the robot disambiguate these two by specifying the triangle color (see Figures~\ref{chap9:fig:director_3} and~\ref{chap9:fig:director_3_h}). Each time, it mentioned ``green storage box'' and not ``storage box'' because there is a pink storage box in the scene, not visible by the camera.

\newpage

\subsection{PR2 as the receiver}

Now, the roles are inverted, the robot becomes the receiver while the human becomes the director. The task starts in the same configuration as before (see Figure~\ref{chap9:fig:expe_config}). With these new roles, the robot will have to understand the instructed action (take, drop or remove which is a take-drop) and to which block the human will refer. This algorithm allowing to understand such instructions was presented in Section~\ref{chap6:subsec:und_comm}.

First, the human orders the robot to ``take the green cube'' (video at $1'35''$). Such a verbal sentence is analyzed by the Google Speech To Text (STT) API and sent to \acrshort{jahrvis} in the form of a text. Then, querying the \acrshort{nlp} component, \acrshort{jahrvis} (the \acrfull{cm}) gets what is the action the human requested, here ``take'' and a \sparql{} query corresponding to the object, here \verb'?0 isA Block, ?0 hasColor green'. This query is merged with what we call the context, \ie in this task, we consider that the human is only talking about the objects on the table on which the shelf is so it is \verb'?0 isAbove table_1'. Thus, the \acrshort{cm} requests Ontologenius the object list corresponding to this merged query, from the human's perspective. As the other green block is only visible by the robot, there is a single block in the list, \ie the one with a blue circle. Then, the action to execute is sent to the \acrfull{aem}. When the human asks the robot to ``drop the cube'', \acrshort{jahrvis} has a special case for drop and place actions, checking that the robot is already holding a block and as it is the case, the action is sent to the \acrshort{aem}.

Then, the second human's order ``remove the block with a circle'' is processed the same way as explained previously, except that the remove action is considered as being a decomposition of the take and drop actions so the robot executes both in a raw.

Finally, for the last block to remove, we can see two mechanisms of contingency handling. First, when the human said ``take the block with a triangle'', the STT returned ``take is about to whip a triangle'' which led the \acrshort{nlp} to return a comprehension score below the expected minimum to the \acrshort{cm}. Therefore, the robot explained that it did not understand the sentence. Then, the human repeated his sentence which was analyzed as explained previously. However, this time there was not one object in the list returned by Ontologenius but two, second contingency. Indeed, there are two blocks with a triangle visible by both the human and the robot as shown in Figure~\ref{chap9:fig:triangles}. This means that the human made an ambiguous request, probably because he lacked of focus. Then, the robot needs to disambiguate the situation. The \acrshort{cm} requests the \acrshort{reg} the verbalization of the two blocks with a triangle, with their minimal set of distinguishable characteristics among the blocks of the context. It allowed the robot to ask ``do you mean the block with a green triangle or the block with a blue triangle?''. When the human answers ``with a blue triangle'', the situation is resolved and as the \acrshort{cm} kept the action to perform in memory, it can then send to the \acrshort{aem} to take the block with a blue triangle. Finally, the block is dropped when it is asked by the human. d


\begin{figure}[ht!]
	\centering
	\includegraphics[width=\textwidth]{figures/chapter4/triangles.png}
	\caption{\label{chap9:fig:triangles} The human asked the robot to take the block with a triangle but there are two blocks with a triangle visible by both the robot and the human.}
\end{figure}




\section{Open challenges for the community }
\label{sec:challenges}

So far, we proposed a cognitive robot architecture handling the Director Task in its simplest form, both 
%for the director and receiver 
roles. In this section, we now present some open challenges for the community around the task. Moreover, because the task can be performed in a controlled environment, we also present in a second part some user studies to investigate ways of sharing information.

\subsection{Some challenges to take up}

\begin{center}
	\begin{tabular}{||l | l ||} 
		\hline
		Challenged abilities / components & Challenges \\ [0.5ex]
		\hline\hline
		Perspective-taking & \ref{chal:cont_analysis}  \\ 
		\hline
		Communication & \ref{chal:change}, \ref{chal:understand}, \ref{chal:words}\\
		\hline
		Task planning & \ref{chal:cont_not_errors}, \ref{chal:cont_errors},  \ref{chal:change} \\
		\hline
		Reference generation & \ref{chal:change}, \ref{chal:spatial_ref}, \ref{chal:past}, \ref{chal:multi} \\
		\hline
		Contingencies handling & \ref{chal:cont_analysis}, \ref{chal:cont_not_errors}, \ref{chal:cont_errors}, \ref{chal:change} \\ [1ex]
		\hline
	\end{tabular}
\end{center}

\begin{enumerate}[leftmargin=* ,parsep=0cm,itemsep=0cm,topsep=0cm]
	\item Fine contingency analysis:  Due to the high ambiguity between the blocks and the presence of occluded compartments, failures can easily arise and have to be handled. In the case the human is the receiver and does not take the instructed block, the robot has to determine the origin of the failure. It could come from a perspective not taken into account either by the director or the receiver, a block description not clear enough, or just an error of the receiver regarding a correct (non-ambiguous) description.\label{chal:cont_analysis}
	\item Not handling contingencies as errors: Based on the example of the previous challenge, the human takes another block than the one instructed but this block could be part of the next ones to take in the plan. In this case, why the robot should try to ``repair'' i.e., make the human takes the instructed block? Maybe it could mention to them that they took the wrong block but it does not matter because this one is also part of the plan. Then, either the robot could re-plan or even better, use a conditional plan and adapt according to the human's actions.\label{chal:cont_not_errors}
	\item Handling errors as errors: Still based on the case where the human takes another block than the one instructed, the robot has to communicate and negotiate with them in order, first to fix the error i.e., put back the block to its original compartment, then adapt its original instruction to make it clearer and improve the chances to have them take the right one.\label{chal:cont_errors}
	\item Changing something when recurrent failures: In case of recurrent failures by the partner, during one interaction session (multiple tasks can be performed in one session) or along with several ones, the robot could try to analyse the origin of the failures and adapt itself to increase the QoI and reduce the failures. It could be through properties' cost adaptation if the partner has some difficulties with certain visual features or communication context adaptation if the partner took the stored blocks into account in its understanding.\label{chal:change}
	\item Allowing spatial references: As explained in section~\ref{chap9:subsec:psycho}, the Director Task is originally a task to test referential communication. Even if the present version asks the participants to not use spatial reference, this rule could be relaxed to study perspective-corrected spatial Referring Expression Generation.\label{chal:spatial_ref}
	\item Understanding the human instructions: In the current implemented version, the robot can only understand a precise vocabulary, being the one describing the blocks in the way we have thought them. In a more natural interaction, humans could use a richer vocabulary, give instruction in multiple steps or have communications not directly linked to the task. During tests for designing the task, it was common to have instructions like ``take the block with a ... triangle. No, rather the one with a green border''. Such complex communications should have to be managed by the robot. \label{chal:understand}
	\item Introducing code words: As presented in Section~\ref{chap9:subsec:material}, the visual features on the blocks have been designed to be able to see landscapes on them, with a little imagination. During the interaction, the robot could thus try to negotiate some coded words in order to be more efficient in the task considering multiple sessions. Being the receiver, it would have to understand the coded words as to be part of a description and remember them. \label{chal:words}
	\item Referring to a past event: When a human performs multiple times the Director Task with the robot, noteworthy events can happen. These events could be recognized and recorded by the robot so it can refer to them when speaking about an object (\eg ``can you take the one you dropped in the previous task ?''). Likewise, the human may also use these past events and the robot would have to understand them. \label{chal:past}
	%We could even imagine not only the robot referring to past events but also the human starting to doing it and the robot understanding these references. \label{chal:past}
	\item Communicating about multiple blocks in a raw: Instead of giving instructions one at a time, the director could give instructions for multiple blocks in a raw. This may bring different kinds of communications from the base task as ``I do not remember the instruction for the last block'' when the human is the receiver. Also, this would be interesting for planning when the robot is the director as it could give instructions such as ``Take all the blocks with a triangle on them'' and it would be a different kind of instructions to interpret when the robot is the receiver.
	\label{chal:multi}
\end{enumerate}


\subsection{Some Director Task-based user studies to perform}

Some robot behaviours, mainly about the referring expression generation, have been designed with regard to the current literature but could be refined thanks to user studies based on the Director Task. The references to the blocks involve the minimum of visual features allowing to discriminate them without ambiguity to fit the Grice's Maxim of Quantity~\citep{grice_1975_logic}. However, due to all the cognitive mechanisms to use in this task (\eg perspective-taking) and the high ambiguity among the blocks, evaluating such behaviour compared to a full explanation could be interesting. Indeed, giving a reference with more information than needed would ensure to not match blocks being only visible by the receiver, which could help them to select the right block.

As presented in section~\ref{chap9:subsec:material}, a special compartment equipped with a wire mesh can be used. Referring to a block matching also the one in this particular compartment could disturb the receiver or at least require a higher cognitive load to determine the right block to take. Such behavior could also be interesting to evaluate. In the same way, a block that was visible by the receiver and that the director move in a hidden compartment could disturb the receiver.

Evaluating such behaviours in a controlled task where the participants cannot know the real goal of the study could help the community in the design of architectures applied to more realistic scenarios.

\section{Conclusion}
This chapter presents a new task inspired by psychology to assess cognitive robot architecture capacities, highlight them and challenge them: the Director Task. This task involves cognitive abilities such as perspective-taking, communication, planning, and contingency handling which are studied a lot in \acrshort{hri}. Along with this task, we describe the cognitive architecture we built. It is currently able to perform the task in the nominal case with the robot being the receiver or the director. 

The Director Task we propose aims to be extended and be a sandbox for the \acrshort{hri} community. We have presented nine possible challenges to be taken up and two possible user studies to be carried out. As a base to be reused, the components of the architecture are released in open-source to anyone who would like to pick few components or to use it in its integrity. From there, new features can be implemented, improving the architecture abilities.

\ifdefined\included
\else
\bibliographystyle{acm}
\bibliography{These}
\end{document}
\fi